#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import re
from multiprocessing.pool import Pool

import requests
from bs4 import BeautifulSoup


def get_shu_links():
#"""获取书的URL链接"""
# url = 'http://www.jb51.net/books/list422_1.html'
url = 'http://www.jb51.net/books/list19_2.html'
response = requests.get(url)
response.encoding = response.apparent_encoding
soup = BeautifulSoup(response.text, 'lxml')
#print(soup)
titles = soup.select('div.fl.w690 > ul.cur-cat-list > li > dl.soft-con > dt.clearfix')
links = soup.select('div.fl.w690 > ul.cur-cat-list > li > a')
for title,link in zip(titles,links):
print('书名：'+(title.get('title'), title.text)[1]+' '+'链接：'+get_detail_links('http://www.jb51.net/'+(link.get('href'), link.text)[0]))
# print('http://www.jb51.net/'+(link.get('href'), link.text)[0])
# print('链接'+get_detail_links('http://www.jb51.net/'+(link.get('href'), link.text)[0]))

def get_detail_links(url):
# url = "http://www.jb51.net//books/491505.html"
response = requests.get(url)
response.encoding = response.apparent_encoding
soup = BeautifulSoup(response.text, 'lxml')
# print(soup.find('a', attrs={'href': re.compile("^http://.*.rar")})['href'])
return soup.find('a', attrs={'href': re.compile("^http://.*.rar")})['href']






if __name__ == '__main__':
get_shu_links()
# get_detail_links()

# pool = Pool(processes=4)
# pool.map(get_detail_links())
# pool.close()
# pool.join()
